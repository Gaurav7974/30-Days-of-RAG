documents = [
    "In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' which introduced the Imitation Game—now famously known as the Turing Test—as a practical criterion for evaluating machine intelligence. The paper challenged the philosophical notion of 'Can machines think?' by reframing it into an operational, observable test where a human interrogator attempts to distinguish between a machine and a human based solely on their textual responses. Turing's work laid the conceptual groundwork for decades of research in artificial intelligence, influencing not only theoretical debates but also early AI architectures, natural language processing, and the ethical frameworks surrounding machine cognition. Despite criticism from philosophers like John Searle (via the Chinese Room argument), the Turing Test remains a cultural and historical benchmark in AI development.",
    
    "The Dartmouth Summer Research Project on Artificial Intelligence, held in 1956 at Dartmouth College, is widely regarded as the founding event of artificial intelligence as a formal scientific discipline. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, the workshop brought together a small group of researchers with the explicit goal of exploring the conjecture that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.' It was during this conference that John McCarthy first coined the term 'artificial intelligence' to distinguish this emerging field from cybernetics and earlier computational theories. The participants made overly optimistic predictions—such as mastering symbolic reasoning within a summer—but the event catalyzed sustained academic and governmental interest, leading to the establishment of dedicated AI labs at MIT, Stanford, and CMU throughout the 1960s.",
    
    "In 1951, Alan Turing, already renowned for his wartime codebreaking and theoretical computer science contributions, designed one of the earliest known computer chess programs. Though no computer at the time had sufficient memory to run it, Turing manually executed the algorithm during a 1952 game against his friend Alick Glennie—losing in 29 moves—but effectively demonstrating the feasibility of machine game-playing. This effort represented one of the first practical applications of heuristic search in AI, using a simplified evaluation function and a two-move lookahead strategy. Turing’s chess program illustrated the core AI principle of simulating human expertise through rule-based symbolic manipulation, foreshadowing later breakthroughs like Arthur Samuel’s checkers player and IBM’s Deep Blue, and establishing games as a critical testing ground for AI reasoning under uncertainty.",
    
    "The Logic Theorist, developed in 1955–1956 by Allen Newell, Herbert A. Simon, and programmer Cliff Shaw, is recognized as the first true artificial intelligence program. Designed to mimic human problem-solving, it could prove mathematical theorems from Whitehead and Russell’s *Principia Mathematica* using symbolic logic and heuristic search. Notably, it found a more elegant proof for Theorem 2.85 than the one originally published—an achievement its creators attempted (unsuccessfully) to publish in the *Journal of Symbolic Logic*. The Logic Theorist introduced foundational AI concepts: symbolic representation, goal-directed search, and the use of heuristics to manage combinatorial explosion. It ran on the JOHNNIAC computer at RAND Corporation and directly inspired the General Problem Solver (GPS), cementing the 'physical symbol system hypothesis' as the dominant AI paradigm for the next two decades.",
    
    "During the late 1950s and early 1960s, early AI research was heavily funded by the U.S. Department of Defense, particularly through the Advanced Research Projects Agency (ARPA, now DARPA), which sought automated systems capable of language translation, battlefield simulation, and decision support. This period saw the rise of symbolic AI, centered on logic, rules, and knowledge representation, with major projects including the General Problem Solver (GPS), the ELIZA chatbot (1966) by Joseph Weizenbaum—which simulated a Rogerian psychotherapist using pattern matching—and early natural language systems like SHRDLU, which could understand commands in a simulated 'blocks world.' Despite initial enthusiasm, progress stalled by the late 1960s due to computational limits, lack of common-sense knowledge, and overpromising, culminating in the first 'AI winter' when funding agencies sharply reduced support after the 1969 Lighthill Report criticized the field’s practical relevance.",
    
    "The perceptron, introduced by Frank Rosenblatt in 1957, was a pioneering neural network model capable of binary classification through weighted inputs and a threshold activation function. Implemented on the IBM 704 and later as a custom hardware machine called the Mark I Perceptron, it could learn to recognize simple visual patterns through supervised training. In 1969, however, Marvin Minsky and Seymour Papert published *Perceptrons*, a critical analysis demonstrating that single-layer networks could not solve non-linearly separable problems like XOR—effectively halting neural network research for over a decade. This setback redirected AI focus toward symbolic methods until the backpropagation algorithm and multi-layer architectures revived neural approaches in the 1980s, ultimately leading to today’s deep learning revolution."
]

query = "What event in 1956 marked the official birth of artificial intelligence as a discipline?"